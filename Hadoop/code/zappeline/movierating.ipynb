{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "### Let's make sure spark is working fine\n",
    "Let's see what verison we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is already initialized in zappeline\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "wget  http://media.sundog-soft.com/hadoop/ml-100k/u.data -O /tmp/u.data\n",
    "wget  http://media.sundog-soft.com/hadoop/ml-100k/u.item -O /tmp/u.item\n",
    "\n",
    "echo \"Downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "hadoop fs -rm -r -f /tmp/ml-100k/\n",
    "hadoop fs mkdir /tmp/ml-100k/\n",
    "hadoop fs -put /tmp/u.data /tmp/ml-100k/\n",
    "hadoop fs -put /tmp/u.item /tmp/ml-100k/\n",
    "# SHift + Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show title -> Copy data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scala is primary interpreter\n",
    "final case class Rating(movieID: Int, rating: Int)\n",
    "\n",
    "val lines = sc.textfile(\"hdfs:///tmp/ml-100k/u.data\").map(x => { val fields = x.split('\\t'); Rating(fields(1).toInt, fields(2).toInt  )})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RDD to Dataframe\n",
    "import sqlContext.implicits._\n",
    "val ratingsDF = lines.toDF()\n",
    "\n",
    "ratingsDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topMovieIDs = ratingsDF.groupBy('movieID').count().orderby(desc(\"count\"))\n",
    "\n",
    "\n",
    "topMovieIDs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in spark 1.6\n",
    "ratingsDF.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM ratings LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT rating, count(*) AS count\n",
    "FROM ratings\n",
    "GROUP BY rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clicking on visulaize will show hsitogram, pie chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final case class Movie(movieID: Int, title: String)\n",
    "\n",
    "val lines = sc.textfile(\"hdfs:///tmp/ml-100k/u.item\").map(x => { val fields = x.split('|'); Movie(fields(0).toInt, fields(1)  )})\n",
    "\n",
    "import sqlContext.implicits._\n",
    "val moviesDF = lines.toDF()\n",
    "\n",
    "moviesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF.registerTempTable(\"titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT t.title, count(*) as cnt\n",
    "FROM ratinsg r\n",
    "JOIN titles t\n",
    "ON r.movieID= t.movieID\n",
    "GROUP BY t.title\n",
    "ORDER BY cnt DESC LIMIT 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long tail"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
